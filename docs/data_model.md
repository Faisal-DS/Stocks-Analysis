# Data Model and Storage Design

## Overview
This project persistently stores stock market time-series data using MongoDB Atlas as a
NoSQL big data storage solution. The data model is designed to support scalable ingestion,
efficient querying, and distributed processing using Apache Spark.

To preserve data lineage and reproducibility, raw ingested data is stored separately from
derived analytical outputs. This separation enables repeated reprocessing and ensures that
all computed results can be traced back to immutable source data.

**Database:** `stocks`

**Collections:**
- `raw_prices` – source-of-truth market data
- `daily_agg` – Spark-derived daily aggregates
- `indicators` – Spark-derived technical and risk indicators

---

## Collection: raw_prices

### Purpose
The `raw_prices` collection stores immutable stock market price bars collected from external
data providers. Each document represents a single market observation for a specific symbol,
timestamp, and interval.

This collection serves as the authoritative source for all downstream analytics.

### Document Schema
- `symbol` (string): Stock ticker symbol (e.g., "AAPL")
- `timestamp` (Date): UTC timestamp representing the start time of the bar
- `interval` (string): Data granularity (e.g., "5min", "1day")
- `open` (double): Opening price
- `high` (double): Highest price
- `low` (double): Lowest price
- `close` (double): Closing price
- `volume` (long/int): Traded volume during the interval
- `source` (string): Data provider identifier (e.g., "alpha_vantage")
- `ingested_at` (Date): UTC timestamp indicating when the record was ingested

### Uniqueness Rule
A compound uniqueness constraint is enforced on:

(symbol, timestamp, interval)

This guarantees that each market bar is stored exactly once and prevents duplicate records
during periodic or repeated ingestion runs.

### Indexes
1. **Unique Index**
{ symbol: 1, timestamp: 1, interval: 1 }

Prevents duplicate inserts and enforces data integrity.

2. **Query Performance Index**
{ symbol: 1, timestamp: -1 }

Optimizes time-range queries, latest-record retrieval, and Spark batch reads.

---

## Collection: daily_agg

### Purpose
The `daily_agg` collection stores daily OHLC and volume summaries generated by Apache Spark.
These summaries are derived from raw intraday and/or daily data and persisted for reuse
in analysis and visualization.

### Document Schema
- `symbol` (string): Stock ticker symbol
- `date` (Date or ISO date string): Trading date (YYYY-MM-DD)
- `open` (double): Daily opening price
- `high` (double): Daily highest price
- `low` (double): Daily lowest price
- `close` (double): Daily closing price
- `volume` (long/int): Total traded volume
- `computed_at` (Date): UTC timestamp when aggregation was computed

### Indexes
- **Unique Index**
{ symbol: 1, date: 1 }

Ensures a single daily summary per symbol per trading day.

---

## Collection: indicators

### Purpose
The `indicators` collection stores technical and risk indicators computed using Apache Spark,
such as moving averages and rolling volatility measures. These indicators support financial
analysis and decision-support use cases.

### Document Schema
- `symbol` (string): Stock ticker symbol
- `date` (Date or ISO date string): Trading date
- `ma_5` (double): 5-day moving average
- `ma_20` (double): 20-day moving average
- `ma_50` (double): 50-day moving average
- `volatility_20` (double): 20-day rolling volatility
- `computed_at` (Date): UTC timestamp when indicators were computed

### Indexes
- **Unique Index**
{ symbol: 1, date: 1 }

Prevents duplicate indicator records for the same symbol and date.

---

## Design Rationale

- **Persistent Storage:** MongoDB Atlas ensures data is stored durably and does not rely on
  in-memory processing.
- **Separation of Concerns:** Raw data is stored independently from derived analytics to
  support reproducibility and reprocessing.
- **Scalability:** The schema supports multi-symbol, multi-interval time-series growth
  without architectural changes.
- **Big Data Alignment:** The model explicitly addresses Volume and Velocity dimensions
  through continuous ingestion and large historical accumulation.
- **Integration Readiness:** The schema and indexing strategy are optimized for distributed
  batch processing using Apache Spark.

Veracity considerations are intentionally excluded from the scope of this project.
